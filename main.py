# main.py

from fastapi import FastAPI, File, UploadFile, Form, HTTPException,Request
from typing import List, Optional
from preprocess import preprocess_vectordbs
from inference import inference
from webscrape import scrape_web_data
import uvicorn
import os, json, validators, pickle
import tldextract
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from pydantic import BaseModel
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings

app = FastAPI()

# Allow frontend CORS origins
origins = [
    "https://rag-chatbot-frontend-three.vercel.app",
    "http://13.60.34.232:8000",
    "https://rag-chatbot-web.shop",
    "http://rag-chatbot-web.shop",
    "http://localhost:3000",
    "https://datalysis-website.vercel.app",
    "https://datalysis.rag-chatbot-web.shop",
    "https://www.genaitechsol.com",
    "https://anugrah-web.vercel.app",
    "https://kunjeshweb.vercel.app",
    "https://demo-rahi.vercel.app"
]

app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

BASE_DIR = os.path.abspath(os.path.dirname(__file__))
BASE_OUTPUT_DIR = os.getenv("BASE_OUTPUT_DIR", os.path.join(BASE_DIR, "projects"))
os.makedirs(BASE_OUTPUT_DIR, exist_ok=True)

print("üß± BASE_OUTPUT_DIR =", BASE_OUTPUT_DIR)
print("üß± Contents of BASE_OUTPUT_DIR:", os.listdir(BASE_OUTPUT_DIR))


session_state = {
    "retriever": None,
    "preprocessing_done": False,
    "index": None,
    "docstore": None,
    "embedding_model_global": None,
    "selected_vectordb": "FAISS",
    "selected_chat_model": None,
    "messages": []
}

# ------------------------ üîß Helper to Rebuild FAISS Retriever ------------------------ #
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import SentenceTransformerEmbeddings

import faiss
import pickle
import os

async def rebuild_faiss_retriever(index_path: str):
    model_name = "sentence-transformers/all-MiniLM-L6-v2"
    embeddings = SentenceTransformerEmbeddings(model_name=model_name)

    abs_index_path = os.path.abspath(index_path)
    index_faiss_path = os.path.join(abs_index_path, "index.faiss")
    index_pkl_path = os.path.join(abs_index_path, "index.pkl")

    # ‚úÖ Proper FAISS load with docstore + mapping
    index = faiss.read_index(index_faiss_path)

    with open(index_pkl_path, "rb") as f:
        store_data = pickle.load(f)

    vectorstore = FAISS(
        embedding_function=embeddings,
        index=index,
        docstore=store_data["docstore"],
        index_to_docstore_id=store_data["index_to_docstore_id"]
    )

    # ‚úÖ Optional debug print
    print("‚úÖ Docstore size:", len(vectorstore.docstore._dict))
    print("‚úÖ Index-to-docstore-id mapping keys (sample):", list(vectorstore.index_to_docstore_id.keys())[:5])

    keys = list(vectorstore.docstore._dict.keys())[:5]
    for i, k in enumerate(keys):
        print(f"[‚úÖ] Doc {i+1} (ID: {k}) preview: {vectorstore.docstore._dict[k].page_content[:120]}")

    return vectorstore.as_retriever()

@app.post("/preprocess")
async def preprocess(
    request: Request,
    project_name: str = Form(None),
    doc_files: Optional[List[UploadFile]] = File(None),
    links: Optional[str] = Form(None),
    embedding_model: str = Form(...),
    chunk_size: int = Form(...),
    chunk_overlap: int = Form(...)
):
    try:
        print("\nüõ†Ô∏è [PREPROCESS] ‚û§ Started preprocessing request.")

        # ‚úÖ Ensure at least one input is provided
        if not doc_files and not links:
            raise HTTPException(status_code=400, detail="‚ùå You must provide at least one PDF or a URL.")

        # ‚úÖ Extract domain folder
        domain = project_name
        domain_folder = os.path.join(BASE_OUTPUT_DIR, domain)
        os.makedirs(domain_folder, exist_ok=True)
        print(f"üìÅ [PREPROCESS] ‚û§ Created/using domain folder: {domain_folder}")

        # ‚úÖ Process links if provided
        links_list = []
        if links:
            try:
                links_list = json.loads(links)
                print(f"üåê [PREPROCESS] ‚û§ Received {len(links_list)} links for scraping.")
                for link in links_list:
                    print(f"üîó Validating link: {link}")
                    if not validators.url(link):
                        raise HTTPException(status_code=400, detail=f"‚ùå Invalid URL: {link}")
            except json.JSONDecodeError:
                raise HTTPException(status_code=400, detail="‚ùå Invalid JSON format for 'links' field.")

        # ‚úÖ Process files if provided
        if doc_files:
            print(f"üìé [PREPROCESS] ‚û§ Uploaded {len(doc_files)} files:")
            for file in doc_files:
                print(f"   ‚îî‚îÄ‚îÄ {file.filename}")
                if file.filename == "":
                    raise HTTPException(status_code=400, detail="‚ùå One of the uploaded files is empty!")

        # ‚úÖ Derive domain if not explicitly provided
        if project_name:
            domain_info = tldextract.extract(project_name)
            domain = f"{domain_info.subdomain + '.' if domain_info.subdomain else ''}{domain_info.domain}.{domain_info.suffix}"
            print(f"üìÇ [PREPROCESS] ‚û§ Using provided project_name as domain: {domain}")
        elif links_list:
            domain_info = tldextract.extract(links_list[0])
            domain = f"{domain_info.subdomain + '.' if domain_info.subdomain else ''}{domain_info.domain}.{domain_info.suffix}"
            print(f"üåç [PREPROCESS] ‚û§ Derived domain from links: {domain}")
        else:
            raise ValueError("Either project_name or links_list must be provided.")

        domain_folder = os.path.join(BASE_OUTPUT_DIR, domain)
        os.makedirs(domain_folder, exist_ok=True)

        # ‚úÖ Scrape web data if links provided
        scraped_data = []
        if links_list:
            try:
                print("üï∏Ô∏è [SCRAPER] ‚û§ Starting web scraping...")
                for link in links_list:
                    scraped_data.extend(await scrape_web_data(link))
                    print(f"‚úÖ [SCRAPER] ‚û§ Scraped content from: {link}")
                with open(os.path.join(domain_folder, "scraped_cache.pkl"), "wb") as f:
                    pickle.dump(scraped_data, f)
                print(f"üíæ [SCRAPER] ‚û§ Scraped data cached at: {domain_folder}")
            except Exception as e:
                print(f"‚ùå [SCRAPER] ‚û§ Web scraping failed: {str(e)}")
                raise HTTPException(status_code=500, detail=f"Web scraping failed: {str(e)}")

        # ‚úÖ Proceed to vector DB preprocessing
        print("üß† [VECTORDB] ‚û§ Calling preprocess_vectordbs...")
        try:
            index, docstore, index_to_docstore_id, vectorstore, retriever, embedding_model_global, pinecone_index_name, vs, qdrant_client = await preprocess_vectordbs(
                doc_files or [], embedding_model, chunk_size, chunk_overlap,
                scraped_data, session_state["selected_vectordb"],
                persist_directory=os.path.join(domain_folder, "faiss_index")
            )
            print(f"üßæ [POST-PROCESS] Docstore preview: {list(docstore._dict.keys())[:5]}")
            print(f"üîó [POST-PROCESS] Index to Docstore ID: {list(index_to_docstore_id.items())[:5]}")
            print("‚úÖ [VECTORDB] ‚û§ Embedding & indexing complete.")

            session_state.update({
                "retriever": retriever,
                "preprocessing_done": True,
                "index": index,
                "docstore": docstore,
                "embedding_model_global": embedding_model_global,
                "pinecone_index_name": pinecone_index_name,
                "vs": vs,
                "qdrant_client": qdrant_client
            })

            state_to_save = session_state.copy()
            state_to_save.pop("retriever", None)
            state_to_save.pop("index", None)
            state_to_save.pop("docstore", None)

            with open(os.path.join(domain_folder, "session_state.pkl"), "wb") as f:
                pickle.dump(state_to_save, f)

            print("üíæ [STATE] ‚û§ Session state saved to:", domain_folder)
            return {"message": f"Preprocessing completed and saved in {domain_folder}"}

        except Exception as e:
            print(f"‚ùå [VECTORDB] ‚û§ Error in preprocess_vectordbs: {str(e)}")
            raise HTTPException(status_code=500, detail=f"Preprocessing failed: {str(e)}")

    except Exception as e:
        print(f"‚ùå [PREPROCESS] ‚û§ Unexpected Error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Unexpected Error: {str(e)}")

@app.post("/select_vectordb")
async def select_vectordb(vectordb: str = Form(...)):
    session_state["selected_vectordb"] = vectordb
    print(f"‚úÖ Selected Vector DB: {vectordb}")
    return {"message": f"Vector DB set to: {vectordb}"}

@app.post("/select_chat_model")
async def select_chat_model(
    chat_model: str = Form(...),
    custom_prompt: str = Form(None),
    project_name: str = Form(None),
    max_output_tokens: int = Form(1024),  
    initial_message: str = Form("Hello! üëã How can I help you today?"),
    chatbot_title: str = Form("AI Chat Assistant"),
    top_k: int = Form(8),  
    temperature: float = Form(0.3)
):
    session_state["selected_chat_model"] = chat_model
    session_state["custom_prompt"] = custom_prompt
    session_state["max_output_tokens"] = max_output_tokens 
    session_state["initial_message"] = initial_message
    session_state["chatbot_title"] = chatbot_title
    session_state["top_k"] = top_k
    session_state["temperature"] = temperature


    print(f"‚úÖ Value of k: {top_k} & Temperature: {temperature}")
    print(f"‚úÖ Chat model set: {chat_model}, Prompt: {custom_prompt}, Max Tokens: {max_output_tokens}")
    print(f"üìù Title: {chatbot_title}, Initial Message: {initial_message}")

    # Use project_name directly
    domain = project_name or "local_upload"
    domain_folder = os.path.join(BASE_OUTPUT_DIR, domain)

    # Load existing session state pickle (if exists)
    session_file = os.path.join(domain_folder, "session_state.pkl")
    if os.path.exists(session_file):
        try:
            with open(session_file, "rb") as f:
                loaded_session = pickle.load(f)
        except:
            loaded_session = {}
    else:
        loaded_session = {}

    # Update loaded session
    loaded_session["selected_chat_model"] = chat_model
    loaded_session["custom_prompt"] = custom_prompt
    loaded_session["max_output_tokens"] = max_output_tokens  # NEW
    loaded_session["initial_message"] = initial_message
    loaded_session["chatbot_title"] = chatbot_title
    loaded_session["top_k"] = top_k
    loaded_session["temperature"] = temperature


    # Save back to file
    os.makedirs(domain_folder, exist_ok=True)
    with open(session_file, "wb") as f:
        pickle.dump(loaded_session, f)

    print(f"üíæ [STATE] ‚û§ Chat model saved in session state at: {session_file}")

    return {"message": "Chat model and max_output_tokens set."}


# ---------------------------- üí¨ CHAT Endpoint ---------------------------- #
class ChatRequest(BaseModel):
    query: str
    project_name: str

@app.post("/chat")
async def chat_with_bot(payload: ChatRequest, request: Request):
    print("\nüí¨ [CHAT] ‚û§ New chat query received.")
    prompt = payload.query
    domain = payload.project_name
    print(f"üßæ [CHAT] ‚û§ Domain: {domain}, Prompt: {prompt}")

    domain_folder = os.path.join(BASE_OUTPUT_DIR, domain)
    session_file = os.path.join(domain_folder, "session_state.pkl")

    if not os.path.exists(session_file):
        print("‚ö†Ô∏è [CHAT] ‚û§ Session file not found.")
        raise HTTPException(status_code=400, detail="‚ùå Session not found. Please preprocess data first.")

    try:
        with open(session_file, "rb") as f:
            loaded_session = pickle.load(f)
        print("‚úÖ [CHAT] ‚û§ Session loaded.")
    except Exception as e:
        print(f"‚ùå [CHAT] ‚û§ Failed to load session state: {e}")
        raise HTTPException(status_code=500, detail="‚ùå Failed to load session state.")

    messages = loaded_session.get("messages", [])
    embedding_model = loaded_session.get("embedding_model_global", None)
    selected_vectordb = loaded_session.get("selected_vectordb", "FAISS")
    selected_chat_model = loaded_session.get("selected_chat_model", None)
    custom_prompt = loaded_session.get("custom_prompt", None)
    top_k = loaded_session.get("top_k", 8)
    temperature = loaded_session.get("temperature", 0.3)


    # ‚úÖ REBUILD and USE the FAISS retriever
    faiss_index_dir = os.path.join(domain_folder, "faiss_index")
    retriever = await rebuild_faiss_retriever(faiss_index_dir)
    print("üì¶ [CHAT] ‚û§ Retriever rebuilt.")

    if not retriever:
        print("‚ùå [CHAT] ‚û§ Retriever not found.")
        raise HTTPException(status_code=500, detail="Retriever not available. Please preprocess data again.")

    # ‚úÖ Debug: test retrieval BEFORE inference
    try:
        docs = retriever.get_relevant_documents(prompt)
        print(f"üîç [DEBUG] ‚û§ Retrieved {len(docs)} documents for query.")
        for i, doc in enumerate(docs):
            snippet = doc.page_content[:200].replace('\n', ' ')
            print(f"üìÑ Doc {i+1}: {snippet}...")
    except Exception as e:
        print(f"‚ùå [DEBUG] ‚û§ Document retrieval failed: {e}")

    # Append user message
    messages.append({"role": "user", "content": prompt})

    # üîÑ Inference call
    try:
        print("üß† [INFERENCE] ‚û§ Calling inference engine...")
        response = inference(
            selected_vectordb,
            selected_chat_model,
            prompt,
            embedding_model,
            messages,
            custom_instructions=custom_prompt,
            faiss_index_dir=faiss_index_dir,
            max_output_tokens=loaded_session.get("max_output_tokens", 1024),
            top_k=top_k,
            temperature=temperature
        )
        print("‚úÖ [INFERENCE] ‚û§ Response generated.")
        messages.append({"role": "assistant", "content": response})
        return {"response": response}
    except Exception as e:
        print(f"‚ùå [INFERENCE] ‚û§ Inference error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Inference error: {str(e)}")

@app.get("/get_config")
async def get_config(project_name: str = "local_upload"):
    import os
    import pickle

    domain_folder = os.path.join(BASE_OUTPUT_DIR, project_name)
    session_file = os.path.join(domain_folder, "session_state.pkl")

    if not os.path.exists(session_file):
        return JSONResponse(
            status_code=404,
            content={"error": "Config not found for this project_name"},
        )

    try:
        with open(session_file, "rb") as f:
            session_data = pickle.load(f)
    except Exception as e:
        return JSONResponse(
            status_code=500,
            content={"error": f"Failed to read session state: {str(e)}"},
        )

    return {
        "chatbot_title": session_data.get("chatbot_title", "AI Chat Assistant"),
        "initial_message": session_data.get("initial_message", "Hello! üëã How can I help you today?"),
        "custom_prompt": session_data.get("custom_prompt", ""),
        "chat_model": session_data.get("selected_chat_model", ""),
        "max_output_tokens": session_data.get("max_output_tokens", 1024),
        "top_k":session_data.get("top_k"),
        "temperature":session_data.get("temperature")
    }


@app.post("/reset")
async def reset_chat():
    session_state.update({
        "retriever": None,
        "preprocessing_done": False,
        "index": None,
        "docstore": None,
        "embedding_model_global": None,
        "messages": []
    })
    print("üîÑ [RESET] ‚û§ Chat session reset.")
    return {"message": "Session reset successfully."}

@app.get("/")
def read_root():
    return {"message": "Hello, World!"}

if __name__ == "__main__":
    port = int(os.getenv("PORT", 8000))
    uvicorn.run(app, host="0.0.0.0", port=port)
